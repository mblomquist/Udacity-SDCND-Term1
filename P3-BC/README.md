# Udacity-SDCND-Behavioral-Cloning-P3
Udacity Self-Driving Car Nanodegree Project 3 - Behavioral Cloning

# Project Description

This project uses a convolutional neural network for end-to-end driving in a simulator, using OpenCV, TensorFlow, and Keras. The process has also be optimized using techniques such as regularization and dropout to generalize the network for driving on multiple tracks.

# TODO List
- [ ] Add shadowing to the augmentation process. 
- [ ] Record a video of the car's performance.

# How to use these files.

### model.py

model.py generates a trained CNN (convolutional neural network) based on input driving images and the related steering control signal.

*Inputs:* 
  - CSV File (data_log/center_driving_log.csv): This CSV file should include the path to images (left, right, and center) and the related steering command. See data_log/ultra_driving_log for an example. This file format should be CSV (MS-DOS). 

*Outputs:* 
  - model.json  - This is a Keras output of the model architecture.
  - model.h5    - This file represents the trained weights of the neural network defined by model.json.
  
 *Command Line Example:*
```
python model.py
```

### drive.py

drive.py utilizes the outpus of model.py to generate a steering control signal for Udacity's Driving Simulator.

*Inputs:*
  - model.json
  - model.h5
  
*Outputs:*
  - Control Signal to Udacity Driving Simulator

*Command Line Example:*
```
python drive.py model.json
```

# Explanation of Everything!

## What's going on!?

### The simple version

The convolutional neural network takes an image or array of images as an input and predicts a steering angle corresponding to the Udacity Drive Simulator. 

### More detailed version 

The convolutional neural network uses a series of convolutional layers to extract features in the form on non-linearities from the input road (driving) image. That data is then flattened into a fully connected layer and reduced in stages to a single value prediction that corresponds to a steering ange. Between the input and output of the network, a number of normalization, filtering, and activation operations are performed to reduce the model's propensity to over fit the input data set and to product a smooth, consistance value between input images during testing. 

## Training Data

The selection of proper training data was particularly crucial for this project and the methodologies I used for the creation and modification of training data have been listed below. The table below provides a brief summary of the training data used for the final version of the project.

| Drive-Type Description | Center Camera | Left Camera | Right Camera | Bias Value |
| --- | --- | --- | --- | --- |
| Center Driving | 7,464 | 7,464 | 7,464 | 0.0 |
| Left-Side Driving | 6,887 | 6,887 | 6,887 | 0.3 |
| Right-Side Driving | 6,725 | 6,725 | 6,725 | -0.3 | 
| Bridge Driving | 1,680 | 1,680 | 1,680 | -0.3 | 
| - | - | - | - | - |
| Total | 22,756 | 22,756 | 22,756 | N/A |

A sample of the raw images data can be seen below.

![preproc](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/IMG_6178.JPG)

### Data Collection

Image data was collected by running the training mode of the Udacity Simulator with a PS4 Controller. During training mode, the simulator would generate images corresponding to the center, left, and right cameras of the car and create a CSV file correlating the file path of these images with the steering command from the PS4 controller at the time of capture. The simulator generates 320 by 160 pixel, .jpg format images and the corresponding steering angle output is in a range of -1 to +1, inclusive. The steering angle output represents the minimum and maximum steering angle of the simulated car and corresponds to a 25° counter-clockwise (-25°) and a 25° clockwise (+25°) rotation of the car’s wheels.

Training data was generated by manually driving the simulated car along the center of the track. In total, a single lap was completed driving at a speed of approximately 8 mph. By using slow, manual driving (the maximum allowable speed was approximately 30 mph), the generated images and steering angles provided smooth transitions through corners and didn’t require hard turns. Additional training data was also generated and the methodology is described in the next section.

### Steering Angle Distribution

The initial sets of training data from the simulator were created without the use of a PS4 controller. Instead, the simulated car was controlled using standard keyboard inputs which generated steering angles in a discrete fashion. Specifically, the steering angles recorded were significantly weighted with three measurements, -1, 0, or +1. Other measurements were recorded in the CSV file for the non-PS4 controller training sets, but these measurements had significantly lower frequencies in the total distribution. Using these training cases, yielded poor performance during autonomous drive testing and were removed from the training data set.

To make the dataset more uniform and have a smoother distribution, a PS4 controller was used for generating the training data. The simple advantage of using the joystick of the PS4 controller, in contrast to a standard keyboard input,  was that the steering angle could be reliably be controlled in an almost continuous manner between -1 and +1 (-25° and +25°). Training data generated using the PS4 controller had a significantly smoother distribution and was centered on 0 (0° CW or CCW) with a very small variance. A histogram of the PS4 training data can be seen below.

![Hist_1](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/hist1.png)  

### Bias Adjustments
While most driving utilizes small steering angles, the car in the simulator still needs to be able to turn around sharp corners and correct itself when wandering around the road. Having a large training data set with a mean of 0 and a small variance was not able to produce this behavior during training, validation, or test. To correct this problem, two additional training data sets were created specifically for driving on the left and right sides of the road. These data sets were created by following or straddling the left- or right-side lane termination lines and adding a bias value to the output steering angle. A bias value of 0.3 was added to the left-side training data which corresponded to a shifting the recorded steering value by 7.5° CW. Similarly, a bias value of -0.3 was added to the right-side training data and corresponds to a steering shift of 7.5° CCW. The 0.3 value was chosen through experimentation as it provided sufficient results during validation and testing. A histogram of the normal and added-bias data can be seen below.

![Hist_2](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/hist2.png) 

### Bridge Driving

In autonomous mode, the model continuously had difficulties with a particular section of the road. This particular section of the road was right before a bridge and the car would continuously leave the road and travel into the lake. To correct this behavior, I attempted to manipulate a variety of hyper-parameters in the model. However, those manipulations never corrected the problem. The problem area of the track can be seen below.

![bridge_1](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/bridge.jpg)

To correct the behavior, I collected additional training data moving at a slow speed through the difficult region of the train. I added these pictures to the Right-Side Training Data pipeline and added a bias of -0.3 to create a 7.5° over-correction to the steering at this point in the road. After running these additional images, the performance of the model during autonomous testing significantly improved. 

## Image Augmentation (Preprocessing)

While having a large amount of training data supported the accuracy of the model, it was important to augment the training data in order to prevent over-fitting and allow the model to perform in environments that are slightly different than those of the training data.

### Brightness Adjustment

The model’s dependency on the brightness of the environment was quite apparent when the autonomous car would steer directly towards shadows during testing. In order to prevent behavior like this, I used a function to randomly scale the brightness of the training image by a factor between 0.75 and 1.25. This technique was borrowed from Vivek Yadav and tended to reduce the model’s desire to be in the darkness. The python function and below/after images can be seen below.

```
def adjust_brightness(image):

      image = cv2.cvtColor(image,cv2.COLOR_BGR2HSV)
      image[:,:,2] = image[:,:,2]*(.25+np.random.uniform())
      image = cv2.cvtColor(image,cv2.COLOR_HSV2RGB)

      return image
``` 

Input Image (un-modified)

![Bright_0](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/input.png)

Brightness Adjusted Image (Reduced Brightness)

![Bright_1](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/bright1.png)

Brightness Adjusted Image (Increased Brightness)

![Bright_2](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/bright2.png)

### Translation

Shifting an image a random number of pixels horizontally and/or vertically is an industry standard form of image augmentation and has previously worked for my classifier in Project 2. The range of linear translation was chosen to be at maximum between 10% and 20% of the width and height of the image. Since the image was not square (input image size was 320x160), I chose to set the translation limit at 20 pixels in any direction. Additionally, I added a factor to correct for the appropriate steering angle with a shifted image.
The python function and the before and after images can be seen below.

```
def transImage(image,label):

      x = np.random.randint(-20,20)
      y = np.random.randint(-20,20)

      M = np.float32([[1,0,x],[0,1,y]])

      image = cv2.warpAffine(image,M,(320,160))
      label = label+(-1*x*.002)

      return image, label
```

Translation Example:

![trans_1](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/trans.png)

### Crop and Resize

During an inspection of the generated simulator images, I noted that there was a significant portion of the image that did not provide any information related to the location on the road. Namely, the horizon in each of the generated images created a significant amount of image data that did not relate to driving condition. To prevent that information from causing noise in the model and to reduce the workload of the model, I chose to crop out most of the horizon. The removal of the lower 60 pixel in the input image did improve the result of the model, but did not generalize well. After reviewing processing techniques by Vivek Yadav, I chose to only remove the low 32 pixels of the image and also to crop out the hood of the car from images.
The python function and example images can be seen below. 

```
def rescale_image(image):

    image = image[32:135, 0:320]
    image = cv2.resize(image,(64,64), interpolation=cv2.INTER_AREA)    
    return image
```

Example 1:

![Crop_1](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/rescale.png)

Example 2:

![Crop_1](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/rescale_img.png)

The function shown above also incorporates a resizing element for the input image. After being cropped, the image is resized to 64 by 64 pixels. The 64x64 size is the input to the neural network and was chosen as it proved a balance between the memory overhead considerations of the full size image and the loss of information with too small of an image.

### Image Flipping

To add additional variability to the training data and also correct for any polar-error induction on my part, I chose to implement an image flipping effect that horizontally inverts the image and multiplies the corresponding steering angle by -1. By performing this operation on only 1/3rd of the training data set (shown in the pipeline section), over-biased data has a high likelihood of being distributed between the left and right-side images. 

The python function and example images are shown below.

```
def flip_image(image,label):

      image = cv2.flip(image,1)
      label = -1*label

      return image, label
```

Input to Flip:

![Flip_0](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/rescale.png)

Flip Example:

![Flip_1](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/flip.png)

### Preprocessing Pipeline

During training, each of the above mentioned augmentation methods may be implemented on a particular image. The pipeline for performing these operations in embedded in the data_gen function that is incorporated with a Keras fit_generator. The generator calculates the start and stop of each image data batch, reads in the image data, and performs the preprocessing pipeline. 
The image data is preprocessed in the following order:


```
img_new, label_new = transImage(img_new,label_new)

img_new = adjust_brightness(img_new)

img_new = rescale_image(img_new)

if batch % 3 == 0:

      img_new, label_new = flip_image(img_new,label_new)
```

The order of operations was chosen based on what order performed best during testing and could be switched without much concern. There was not significant variability in performance depending on the order of preprocessing steps.

## Neural Network
### Model Architecture

To implement behavioral cloning, I utilized a convolutional neural network with a similar architecture to that described by [Bojarski et. al](https://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf). The model consists of 14 total layers and implements a 50% dropout layer after each convolutional set. Additionally, another 50% dropout layer was added after the second fully connected layer. Each of the convolutional layers and the fully connected layers, with exception to the output layer, utilize an exponential rectified linear unit (ELU) to provide non-linearity throughout the model. The ELU activation was chosen primarily for its normaization properties as described by [Clevert et. al](https://arxiv.org/pdf/1511.07289v5.pdf). 

The input image is fed into a normalization layer, similar to that of Bojarski, that reduces the pixel information of the image to a range of ±0.5. This improve the convergence when using the Adam optimizer as described in literature and in my own tests. After normalization, the model implements a set of 3 convolutional layers with a filter kernal of 5x5 and variable depths. The first convolutional layer uses a stride of 2x2 and the remaining layers utilize 1x1 strides. This deviation from the model of Bojarski was mainly to introduce more tunable parameters into the model and achieve a better fit of the data. After the three convolutional layers, a 50% dropout layer is introduced to prevent over fitting the model to the particular dataset. 

The next set of convolutional layers implement a 3x3 filter kernal and continue increasing the depth of the network. These convolutional layers also use a stride of 1x1 and dropout is again added after the two convolutional layers. The use of dropout is, again, to reduce over fitting.

After the 2nd convolutional set, the network is flattened into a fully connected layer and reduced to a size of 100. A dropout layer is implemented between FC layers 100 and 50 to reduce over fitting in the model. During testing, dropout in this location tended to minimize the difference between training cases and validation cases. Following the dropout layer, two more fully connected layers of size 50 and 10 are used with ELU activations and fed into the output layer of the model. 

A simplified view of the model can be seen below:

![model_arc](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/model_arc.PNG)

The Keras model summary can be seen here:
```
Layer (type)                     Output Shape          Param #     Connected to
====================================================================================================
lambda_1 (Lambda)                (None, 64, 64, 3)     0           lambda_input_1[0][0]
____________________________________________________________________________________________________
activation_1 (Activation)        (None, 64, 64, 3)     0           lambda_1[0][0]
____________________________________________________________________________________________________
layer1_conv2d (Convolution2D)    (None, 30, 30, 3)     228         activation_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation)        (None, 30, 30, 3)     0           layer1_conv2d[0][0]
____________________________________________________________________________________________________
layer2_conv2d (Convolution2D)    (None, 13, 13, 24)    1824        activation_2[0][0]
____________________________________________________________________________________________________
activation_3 (Activation)        (None, 13, 13, 24)    0           layer2_conv2d[0][0]
____________________________________________________________________________________________________
layer3_conv2d (Convolution2D)    (None, 9, 9, 36)      21636       activation_3[0][0]
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 9, 9, 36)      0           layer3_conv2d[0][0]
____________________________________________________________________________________________________
activation_4 (Activation)        (None, 9, 9, 36)      0           dropout_1[0][0]
____________________________________________________________________________________________________
layer4_conv2d (Convolution2D)    (None, 7, 7, 48)      15600       activation_4[0][0]
____________________________________________________________________________________________________
activation_5 (Activation)        (None, 7, 7, 48)      0           layer4_conv2d[0][0]
____________________________________________________________________________________________________
layer5_conv2d (Convolution2D)    (None, 5, 5, 64)      27712       activation_5[0][0]
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 5, 5, 64)      0           layer5_conv2d[0][0]
____________________________________________________________________________________________________
activation_6 (Activation)        (None, 5, 5, 64)      0           dropout_2[0][0]
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 1600)          0           activation_6[0][0]
____________________________________________________________________________________________________
activation_7 (Activation)        (None, 1600)          0           flatten_1[0][0]
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 100)           160100      activation_7[0][0]
____________________________________________________________________________________________________
dropout_3 (Dropout)              (None, 100)           0           dense_1[0][0]
____________________________________________________________________________________________________
activation_8 (Activation)        (None, 100)           0           dropout_3[0][0]
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 50)            5050        activation_8[0][0]
____________________________________________________________________________________________________
activation_9 (Activation)        (None, 50)            0           dense_2[0][0]
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 10)            510         activation_9[0][0]
____________________________________________________________________________________________________
dense_4 (Dense)                  (None, 1)             11          dense_3[0][0]
====================================================================================================
Total params: 232,671
Trainable params: 232,671
Non-trainable params: 0
____________________________________________________________________________________________________
```

### Fit Generator

In order to perform training on a large image dataset (> 67,000 images), I implemented the Keras fit_generator function. The fit generator allows the image data preprocessing to be run in parallel with the training phase of the neural network, which ended up reducing the overall run time for the program. In previous testing, I noted that the Keras ImageDataGenerator function increased the overall run time of the model and that still holds for the new architecture. However, the Keras ImageDataGenerator function required data to be fed into the generator and fit in order to perform specific analysis. This differs from the customer generator created as the image preprocessing performed can be operated on single images rather than the entire dataset. The python implementation of this generator can be seen below.

```
def data_gen(batch_size, img_list, label_list):

      batch_image = np.zeros((batch_size, 64, 64, 3))
      batch_label = np.zeros(batch_size)

      j = 0

      img_list, label_list = shuffle(img_list, label_list)

      while 1:

            i = 0

            batch_start = batch_size*j
            batch_stop = batch_size*(j+1)-1

            if batch_stop >= len(img_list)-1:
                  batch_stop = len(img_list)-1

                  j = 0

            for batch in range(batch_start,batch_stop,1):

                  img_new = cv2.imread(img_list[batch])
                  label_new = np.float32(label_list[batch])

                  img_new, label_new = transImage(img_new,label_new)

                  img_new = adjust_brightness(img_new)

                  img_new = rescale_image(img_new)

                  if batch % 3 == 0:

                        img_new, label_new = flip_image(img_new,label_new)

                  batch_image[i] = img_new
                  batch_label[i] = label_new

                  i = i+1

            j = j+1

            yield (batch_image, batch_label)
```

### Training and Validation

When training the model, each of the input images were fed through the pre-processing pipeline to create augmented images. The augmentations, as mentioned earlier, consisted of brightness adjustment, translational shifts, rescaling, and possible image shifting. To provide a visual reference of the preprocessing pipeline, I've included the pre- and post-processing images below.

Example 1 (Left - Unprocessed Image, Right - Processed Image):
![pre-post_01](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/IMG_6179.JPG)

Example 2 (Left - Unprocessed Image, Right - Processed Image):
![pre-post_02](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/IMG_6180.JPG)

Example 3 (Left - Unprocessed Image, Right - Processed Image):
![pre-post_03](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/IMG_6181.JPG)

Example 4 (Left - Unprocessed Image, Right - Processed Image):
![pre-post_04](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/IMG_6182.JPG)

Example 5 (Left - Unprocessed Image, Right - Processed Image):
![pre-post_05](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/IMG_6183.JPG)

As mentioned in the previous section, the Keras fit_generator function was implemented to train the large dataset used for this project. A batch size of 512 images was chosen and the model was run for a total of 3 epochs. The batch size was governed by the limitations of the physical hardware I was using to run this model. At 512 images, the computers resources were adequately used. The number of EPOCHs for the model was chosen based on reviewing the loss minimization at the conclusion of each EPOCH. After 3 EPOCHs, the loss did not significantly reduce and with greater than 7 EPOCHs, the model's loss started increasing. 

The loss function used for the model was mean squared error (MSE) and an Adam optimizer was used to fit the data. The appropriate steering angle could be a range of values for the particular input, so the MSE loss function provided appropriate results. The Adam optimizer was used with default values as it has previously provided good results.

The results for a typical training sequence can be seen below:

```
Epoch 1/3
68096/67980 [==============================] - 646s - loss: 0.1289 - val_loss: 0.0755
Epoch 2/3
68096/67980 [==============================] - 194s - loss: 0.0745 - val_loss: 0.0692
Epoch 3/3
68096/67980 [==============================] - 249s - loss: 0.0682 - val_loss: 0.0636
```

### Test and Verification

Once the model has been trained, I perform a quick verification of the model outputs to 10 images the model hasn't seen prior. These images are provided below for reference and consist of locations on the track where a left or right turn is most certainly required. Instead of measuring the loss between a predicted value and the training steering label, I chose to verify the direction of the imposed turn is correct. For example, the first test image should result in a left turn. Therefore, I would verify the model's prediction is negative. 

A typical output of the model after training is shown here and the corresponding to the images are shown below.

```
[[-0.25916272]
 [-0.29927137]
 [-0.34775543]
 [ 0.29678279]
 [ 0.33453175]
 [-0.26667324]
 [-0.1245453 ]
 [-0.24995117]
 [-0.34802842]
 [-0.04771864]]
```
Test Image 1 (Prediction: -0.25916272 | 6.48° CCW Turn)
![Test_1](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/test_00.jpg)

Test Image 2 (Prediction: -0.29927137 | 7.48° CCW Turn)
![Test_2](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/test_01.jpg)

Test Image 3 (Prediction: -0.34775543 | 8.69° CCW Turn)
![Test_3](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/test_02.jpg)

Test Image 4 (Prediction: 0.29678279 | 7.42° CW Turn)
![Test_4](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/test_03.jpg)

Test Image 5 (Prediction: 0.33453175 | 8.36° CW Turn)
![Test_5](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/test_04.jpg)

Test Image 6 (Prediction: -0.26667324 | 6.67° CCW Turn)
![Test_6](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/test_05.jpg)

Test Image 7 (Prediction: -0.1245453 | 3.11° CCW Turn)
![Test_7](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/test_06.jpg)

Test Image 8 (Prediction: -0.24995117 | 6.25° CCW Turn)
![Test_8](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/test_07.jpg)

Test Image 9 (Prediction: -0.34802842 | 8.70° CCW Turn)
![Test_9](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/test_08.jpg)

Test Image 10 (Prediction: -0.04771864 | 1.19° CCW Turn)
![Test_10](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/test_09.jpg)

### Performance 

The model performs well, with a few areas that could easily become problem areas. Nevertheless, the car was able to complete 5 laps at a resolution of 800x600. After 5 laps, the simulation was stopped. For reference, I added a screen shot of one of the close calls. 

![close 2](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/close2.PNG)

At a later time, I will add a video of the performance.

### Generalization

In order to verify that the model and weights output by the program are not tested specifically for a certain environment, I chose to perform a number of tests using different situations. These situations are listed below.

#### Resolution

I chose to test the model with multiple resolutions of the simulator in order to verify that the results are non-dependent. I was able to successfully test the model from a resolution of 320x240 up to 1024x768 (I stoped after 1024x768 as it seemed appropriate for testing the model. Larger resolutions become more demanding on the hardware running the application). I noticed that the model's performance slightly degraded between 800x600 and 1024x768, but was still able to run multiple laps around the track. The performance degradation, I expect, is a result of computational load on the hardware.

#### Speed

To further test the generalization of the model's architecture and weights, I performed multiple autonomous driving tests using throttle speeds from 0.1 to 0.4. At lower throttles, the performance of the model is improved and the car is able to perform well throughout the entirety of the track. As the throttle is increased to the maximum (0.4), the performance slightly degrades. This seems to be associated with the frame rate output and computational demand of running the simulator and neural network in parallel. As the distance traveled by the car increases between frames, the model has less time to update the steering value resulting in more aggresive driving. This observation also alligns with my previous observations that the model's performance degrades with as the performance of the machine running the model decreases. 

![wild_1](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/wild.PNG)

#### Track 2

My final validation test was to run the simulator on the 2nd track. None of the image data used to develop and train the model consisted of driving on track 2. Therefore, performance on the 2nd track would be a good demonstration of generalization. Performance for the track was subpar as the car crashed about 25% of the way through the track. However, the crash location seems to have a significant shadow, which was likely not trained for with the brightness adjustment.

![wild_2](https://github.com/mblomquist/Udacity-SDCND-Behavioral-Cloning-P3/blob/master/pictures/wild2.PNG)

# Final Thoughts

This is a great project and a wonderful learning experience. I would like to spend more time working out the biasing, adding more shadowing to the preprocessing layer, and improving the performance on track 2. 
